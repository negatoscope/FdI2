<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Fundamentos de Investigación II</title>
  <meta name="description" content="Apuntes de la asignatura de Fundamentos de Investigación II" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Fundamentos de Investigación II" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Apuntes de la asignatura de Fundamentos de Investigación II" />
  <meta name="github-repo" content="negatoscope/FdI2" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Fundamentos de Investigación II" />
  
  <meta name="twitter:description" content="Apuntes de la asignatura de Fundamentos de Investigación II" />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Luis Eudave" />


<meta name="date" content="2020-10-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="estimation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Fundamentos de Investigación II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1</b> Introducción a la probabilidad</a><ul>
<li class="chapter" data-level="1.1" data-path="probability.html"><a href="probability.html#probstats"><i class="fa fa-check"></i><b>1.1</b> ¿Cómo de diferentes son la probabilidad y la estadística?</a></li>
<li class="chapter" data-level="1.2" data-path="probability.html"><a href="probability.html#probmeaning"><i class="fa fa-check"></i><b>1.2</b> ¿Qué significa la probabilidad?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="probability.html"><a href="probability.html#la-vision-frecuentista"><i class="fa fa-check"></i><b>1.2.1</b> La visión frecuentista</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability.html"><a href="probability.html#la-vision-bayesiana"><i class="fa fa-check"></i><b>1.2.2</b> La visión bayesiana</a></li>
<li class="chapter" data-level="1.2.3" data-path="probability.html"><a href="probability.html#cual-es-la-diferencia-y-quien-tiene-razon"><i class="fa fa-check"></i><b>1.2.3</b> ¿Cuál es la diferencia? ¿Y quién tiene razón?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>1.3</b> Teoría de probabilidad básica</a><ul>
<li class="chapter" data-level="1.3.1" data-path="probability.html"><a href="probability.html#introduccion-a-las-distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>1.3.1</b> Introducción a las distribuciones de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>1.4</b> La distribución binomial</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probability.html"><a href="probability.html#introduccion-al-binomio"><i class="fa fa-check"></i><b>1.4.1</b> Introducción al binomio</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>1.5</b> La distribución normal</a><ul>
<li class="chapter" data-level="1.5.1" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>1.5.1</b> Densidad de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>1.6</b> Otras distribuciones útiles</a></li>
<li class="chapter" data-level="1.7" data-path="probability.html"><a href="probability.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>2</b> Estimación</a><ul>
<li class="chapter" data-level="2.1" data-path="estimation.html"><a href="estimation.html#srs"><i class="fa fa-check"></i><b>2.1</b> Muestras, poblaciones y muestreo</a><ul>
<li class="chapter" data-level="2.1.1" data-path="estimation.html"><a href="estimation.html#pop"><i class="fa fa-check"></i><b>2.1.1</b> Definición de una población</a></li>
<li class="chapter" data-level="2.1.2" data-path="estimation.html"><a href="estimation.html#muestras-aleatorias-simples"><i class="fa fa-check"></i><b>2.1.2</b> Muestras aleatorias simples</a></li>
<li class="chapter" data-level="2.1.3" data-path="estimation.html"><a href="estimation.html#la-mayoria-de-las-muestras-no-son-muestras-aleatorias-simples"><i class="fa fa-check"></i><b>2.1.3</b> La mayoría de las muestras no son muestras aleatorias simples</a></li>
<li class="chapter" data-level="2.1.4" data-path="estimation.html"><a href="estimation.html#cuanto-importa-si-no-se-tiene-una-muestra-aleatoria-simple"><i class="fa fa-check"></i><b>2.1.4</b> ¿Cuánto importa si no se tiene una muestra aleatoria simple?</a></li>
<li class="chapter" data-level="2.1.5" data-path="estimation.html"><a href="estimation.html#parametros-poblacionales-y-estadisticos-muestrales"><i class="fa fa-check"></i><b>2.1.5</b> Parámetros poblacionales y estadísticos muestrales</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="estimation.html"><a href="estimation.html#lawlargenumbers"><i class="fa fa-check"></i><b>2.2</b> La ley de los grandes números</a></li>
<li class="chapter" data-level="2.3" data-path="estimation.html"><a href="estimation.html#samplesandclt"><i class="fa fa-check"></i><b>2.3</b> Distribuciones muestrales y el teorema del límite central</a><ul>
<li class="chapter" data-level="2.3.1" data-path="estimation.html"><a href="estimation.html#samplingdists"><i class="fa fa-check"></i><b>2.3.1</b> Distribución muestral de la media</a></li>
<li class="chapter" data-level="2.3.2" data-path="estimation.html"><a href="estimation.html#existen-distribuciones-muestrales-para-cualquier-estadistico-muestral"><i class="fa fa-check"></i><b>2.3.2</b> Existen distribuciones muestrales para cualquier estadístico muestral</a></li>
<li class="chapter" data-level="2.3.3" data-path="estimation.html"><a href="estimation.html#clt"><i class="fa fa-check"></i><b>2.3.3</b> El teorema del límite central</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="estimation.html"><a href="estimation.html#pointestimates"><i class="fa fa-check"></i><b>2.4</b> Estimando parámetros poblacionales</a><ul>
<li class="chapter" data-level="2.4.1" data-path="estimation.html"><a href="estimation.html#estimando-la-media-poblacional"><i class="fa fa-check"></i><b>2.4.1</b> Estimando la media poblacional</a></li>
<li class="chapter" data-level="2.4.2" data-path="estimation.html"><a href="estimation.html#estimando-la-desviacion-estandar-poblacional"><i class="fa fa-check"></i><b>2.4.2</b> Estimando la desviación estándar poblacional</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="estimation.html"><a href="estimation.html#ci"><i class="fa fa-check"></i><b>2.5</b> Estimando un intervalo de confianza</a><ul>
<li class="chapter" data-level="2.5.1" data-path="estimation.html"><a href="estimation.html#que-pasa-cuando-no-conocemos-los-parametros-poblacionales"><i class="fa fa-check"></i><b>2.5.1</b> ¿Qué pasa cuando no conocemos los parámetros poblacionales?</a></li>
<li class="chapter" data-level="2.5.2" data-path="estimation.html"><a href="estimation.html#interpretando-un-intervalo-de-confianza"><i class="fa fa-check"></i><b>2.5.2</b> Interpretando un intervalo de confianza</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="estimation.html"><a href="estimation.html#samplesize"><i class="fa fa-check"></i><b>2.6</b> Estimando el tamaño muestral</a></li>
<li class="chapter" data-level="2.7" data-path="estimation.html"><a href="estimation.html#resumen-1"><i class="fa fa-check"></i><b>2.7</b> Resumen</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Investigación II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Fundamentos de Investigación II</h1>
<p class="author"><em>Luis Eudave</em></p>
<p class="date"><em>2020-10-23</em></p>
</div>
<div id="probability" class="section level1">
<h1><span class="header-section-number">Capítulo 1</span> Introducción a la probabilidad</h1>
<p>Para muchas personas, cuando piensan en estadística se les viene esto a la mente: calcular promedios, recopilar datos, elaborar gráficos y ponerlos todos en un informe en algún lugar. Mas o menos como coleccionar sellos o cucharillas, pero con números. Sin embargo, las estadística cubre mucho más que eso. De hecho, la estadística descriptiva es una de las partes más pequeñas de la estadística, y una de las menos poderosas (en cuanto a las conclusiones que puede aportar). La parte más importante y más útil de la estadística es aquella que que permite hacer <em>inferencias</em> sobre los datos.</p>
<p>Una vez contemplada las estadística en estos términos, -que la estadística está ahí para ayudarnos a hacer inferencias a partir de datos- podemos ver ejemplos de ella en todas partes. Por ejemplo, aquí hay un pequeño extracto de un periódico mexicano:</p>
<blockquote>
<p>“Tengo un trabajo difícil”, dijo el Presidente mexicano en respuesta a una encuesta que encontró que su gobierno ha pasado de gozar los mayores índices de popularidad en la historia (&gt;70%) a un 38 por ciento.</p>
</blockquote>
<p>Este tipo de comentarios suele pasar como completamente irrelevante en los periódicos o en la vida cotidiana, pero pensemos brevemento sobre lo que implica. Una compañía encuestadora ha realizado una encuesta, presumiblemente una muy grande porque tiene los medios y puede permitírselo. Imaginemos que llamaron a 1.000 votantes al azar, y 380 (38%) de ellos afirmaron que tenían la intención de votar por el Presidente. En las últimas elecciones federals, la Instituto Electoral Mexicano confirmó la participación de 56.611.027 votantes; por lo tanto, las opiniones de los 56.610.027 votantes restantes (aproximadamente el 99.998% de los votantes) siguen siendo desconocidas para la encuestadora (y para nosotros). Aún suponiendo que nadie mintió en la encuesta, lo único que podemos decir con un 100% de seguridad es que el verdadero voto primario al Presidente está en algún lugar entre 380/56.611.027 (aproximadamente el 0.0007%) y 56.610.307/56.611.027 (alrededor del 99.9987%), lo cual no aporta mucho. Entonces, ¿sobre qué base es legítimo para la empresa encuestadora, el periódico y el lectores concluir que la intención de voto al Presidente mexicano fue de sólo el 38%?</p>
<p>La respuesta a la pregunta es bastante obvia: si llamo a 1.000 personas al azar, y 380 de ellas dicen tienen la intención de votar por el Presidente, entonces parece muy poco probable que estas sean las <em>únicas</em> 380 personas del todo el público votante que realmente tiene la intención de hacerlo. En otras palabras, suponemos que los datos recopilados por la empresa encuestadora son bastante representativos de la población en general. ¿Pero qué tan representativo? ¿Nos sorprendería descubrir que la verdadera intención de voto es en realidad del 34%? ¿39%? ¿57%? En este punto nuestración intuición comienza a romperse un poco. Nadie se sorprendería si fuese el 34%, y todos lo harían con un 57%, pero es un poco difícil decir si el 39% es plausible. Necesitamos herramientas más poderosas que solo el mirar los números y adivinar.</p>
<p><strong><em>La estadística inferencial</em></strong> proporciona las herramientas que necesitamos para responder a este tipo de preguntas, y ya que este tipo de preguntas se encuentran en el corazón del quehacer científico, ocupan una parte sustancial de cada curso introductorio sobre estadística y métodos de investigación. Sin embargo, la teoría sobre la estadística inferencial está construida sobre la <strong><em>teoría de probabilidad</em></strong>. Y es a la teoría de la probabilidad a la que ahora debemos prestar atención. La discusión de la teoría de la probabilidad en esta asignatura será básicamente de fondo: no hay mucho contenido estadístico <em>per se</em> en este capítulo. Sin embargo, debido a que gran parte de la estadística se sustenta en la teoría de la probabilidad, merece la pena ir cubriendo algunos de los conceptos básicos.</p>
<div id="probstats" class="section level2">
<h2><span class="header-section-number">1.1</span> ¿Cómo de diferentes son la probabilidad y la estadística?</h2>
<p>Antes de comenzar a hablar sobre la teoría de la probabilidad, es útil pensar un momento en la relación que existe entre probabilidad y estadística. Las dos disciplinas están estrechamente relacionadas, pero no son idénticas. La teoría de la probabilidad es “la doctrina de las posibilidades”. Es una rama de las matemáticas que te dice con qué frecuencia sucederán diferentes tipos de eventos. Por ejemplo, todas estas preguntas son cosas que puedes responder usando la teoría de la probabilidad:</p>
<ul>
<li>¿Cuáles son las probabilidades de que al lanzar una moneda salga cara 10 veces seguidas?</li>
<li>Si lanzo dos dados de seis caras, ¿qué probabilidad hay de que tire dos seises?</li>
<li>¿Qué probabilidad hay de que cinco cartas extraídas de un mazo perfectamente barajado sean todas de corazones?</li>
<li>¿Cuál es la probabilidad de que gane la lotería?</li>
</ul>
<p>Hay que tener en cuenta que todas estas preguntas tienen algo en común. En cada caso, existe y se conoce una “verdad sobre el mundo”, y la pregunta se refiere más bien al “qué tipo de eventos” sucederán. En la primera pregunta que <em>sé</em> que la moneda es justa (no es más pesada por uno de los lados, sesgando el resultado), por lo que hay un 50% de probabilidad de que cualquier lanzamiento de moneda salga cara. En la segunda pregunta, <em>sé</em> que la probabilidad de sacar un 6 en un solo dado es de 1 en 6. En la tercera pregunta <em>sé</em> que la baraja se baraja correctamente (no hay un acomodo de cartas). Y en la cuarta pregunta, <em>sé</em> que la lotería sigue unas reglas específicas. El punto clave aquí es que las preguntas probabilísticas comienzan con un <strong><em>modelo</em></strong> conocido del mundo, y usamos ese modelo para hacer algunos cálculos. El modelo subyacente puede ser bastante simple. Por ejemplo, en el ejemplo de lanzar monedas, podemos escribir el modelo de esta manera: <span class="math display">\[
P(\mbox{cara}) = 0.5
\]</span> que puedes leer como “la probabilidad de que salga cara es 0.5”. Como veremos más adelante, de la misma manera que los porcentajes son números que van del 0% al 100%, las probabilidades son solo números que van del 0 al 1. Cuando usamos este modelo de probabilidad para responder a la primera pregunta, en realidad no sé exactamente qué va a ocurrir. Tal vez obtenga 10 caras, como dice la pregunta. Pero quizás salgan sólo tres caras. Esta es la clave: con la teoría de la probabilidad, se conoce el <em>modelo</em>, pero no los <em>datos</em>.</p>
<p>Hemos visto lo que es la probabilidad. ¿Qué hay de la estadística? Las preguntas en estadística funcionan al revés. En estadística, nosotros <em>no</em> sabemos la verdad sobre el mundo. Todo lo que tenemos son los datos, y es a partir de esos datos que queremos <em>aprender</em> sobre la verdad del mundo. Las preguntas estadísticas tienden a parecerse más a estas:</p>
<ul>
<li>Si mi amigo lanza una moneda 10 veces y obtiene 10 caras, ¿me está engañando?</li>
<li>Si las primeras cinco cartas de la parte superior del mazo son todas de corazones, ¿qué tan probable es que se haya barajado el mazo?</li>
<li>Si el hijo del comisionado de la lotería gana la lotería, ¿qué tan probable es que el sorteo esté amañado?</li>
</ul>
<p>Esta vez, lo único que tenemos son datos. Lo que <em>sé</em> es que vi a mi amigo lanzar la moneda 10 veces y salió cara en cada una de las veces. Y lo que quiero es <strong><em>inferir</em></strong> si debería concluir que lo que acabo de ver es en realidad una moneda justa lanzada 10 veces seguidas, o si debería sospechar que mi amigo me está jugando una mala pasada. Los datos que tengo se ven así (cada C es una cara):</p>
<pre><code>C C C C C C C C C C C</code></pre>
<p>y lo que estoy tratando de hacer es averiguar en qué “modelo de verdad del mundo” debería confiar. Si la moneda es justa, entonces el modelo que debo aceptar es uno que diga que la probabilidad de que salga cara es 0.5; es decir, <span class="math inline">\(P(\mbox{cara}) = 0.5\)</span>. Si la moneda no es justa, entonces debo concluir que la probabilidad de que salga cara <em>no</em> es 0.5, lo cual escribiríamos como <span class="math inline">\(P(\mbox{cara}) \neq 0.5\)</span>. En otras palabras, el objetivo de la inferencia estadística es decidir cual de estos modelos de probabilidad es el correcto. Vemos pues, que una pregunta en estadística no es la misma que una pregunta en probabilidad, pero están íntimamente conectados entre sí. Es por ello que una buena introducción a la teoría estadística comenzará con una discusión sobre qué es la probabilidad y cómo funciona.</p>
</div>
<div id="probmeaning" class="section level2">
<h2><span class="header-section-number">1.2</span> ¿Qué significa la probabilidad?</h2>
<p>Comencemos con la primera de estas preguntas. ¿Qué es la “probabilidad”? Puede parecer sorprendente, pero mientras que los estadísticos y matemáticos (en su mayoría) están de acuerdo sobre cuáles son las <em>reglas</em> de la probabilidad, hay mucho menos consenso sobre lo que realmente <em>significa</em> la palabra. Parece extraño porque todos usemos con soltura palabras como “posibilidad”, “probabilidad”, “posible” y “probable”, y además no parece que deba ser una pregunta difícil de responder. Si tuvieramos que explicar el concepto de “probabilidad” a un niño de cinco años, podríamos hacerlo sin muchos problemas. Pero si alguna vez lo has intentando en la vida real, podrías terminar esa conversación sintiendo que no lo has hecho muy bien y que (como con muchos conceptos cotidianos) resulta que <em>realmente</em> no sabemos de qué se trata.</p>
<p>Aún así, vamos a intentarlo. Supongamos que quiero apostar en un juego de fútbol entre dos equipos que en lugar de humanos están formados por robots: el <em>Tesla FC</em> y el <em>Real TikTok</em>. Según los datos de esta temporada, calculo que hay un 80% de probabilidad de que el <em>Tesla FC</em> gane. ¿Qué quiero decir realmente con eso? Tenemos tres posibilidades…</p>
<ul>
<li>Son equipos de robots, así que puedo hacer que jueguen una y otra vez, y si lo hiciera, el <em>Tesla FC</em> ganaría 8 de cada 10 juegos de media.</li>
<li>En cada juego, sólo estaría de acuerdo en que apostar en este juego es “justo” si una apuesta de $1 al <em>Real TikTok</em> da un pago de $5 (es decir, recibo mi $1 de vuelta más una recompensa de $4 por haber acertado), al igual que una apuesta de $4 al <em>Tesla FC</em> también da un pago de $5 (es decir, mi apuesta de $4 más una recompensa de $1).</li>
<li>Mi “creencia” o “confianza” subjetiva en una victoria del <em>Tesla FC</em> es cuatro veces más fuerte que mi “creencia” o “confianza” en una victoria del <em>Real TikTok</em>.</li>
</ul>
<p>Cada uno de estos enunciados parece correcto. Sin embargo, no son idénticos, y no todos los estadísticos respaldarían todos ellos por igual. La razón es que hay diferentes ideologías o visiones estadísticas y dependiendo de cual se escoja, se podría decir que algunas de estas declaraciones no tienen sentido o que son irrelevantes. En esta sección, se dará una breve introducción a dos de los enfoques principales que existen en la literatura estadística. Esto no significa que sean los únicos enfoques, pero sí los dos más importantes.</p>
<div id="la-vision-frecuentista" class="section level3">
<h3><span class="header-section-number">1.2.1</span> La visión frecuentista</h3>
<p>El primero de los dos enfoques principales a la teoría de la probabilidad, y el más dominante en estadística, se le conoce como la <strong><em>visión frecuentista</em></strong>, y define a la probabilidad como una <strong>_ frecuencia a largo plazo_</strong>. Supongamos que queremos intentar lanzar una moneda justa, una y otra vez. Por definición, esta es una moneda que tiene una <span class="math inline">\(P(Cara) = 0.5\)</span>. ¿Qué resultado podremos observar? Una posibilidad es que los primeros 20 lanzamientos se vean así (donde C es cara y X cruz):</p>
<pre><code>X,C,C,C,C,X,X,C,C,C,C,X,C,C,X,X,X,X,X,C</code></pre>
<p>En este caso, en 11 de los 20 lanzamientos (55%) salió cara. Ahora supongamos que he ido guardando un registro con el número de caras (que llamaré <span class="math inline">\(N_C\)</span>) que han salido, a lo largo de las primeras <span class="math inline">\(N\)</span> lanzadas de moneda, además de calcular la proporción de caras <span class="math inline">\(N_C / N\)</span> con cada registro. Este es el resultado que obtendría:</p>
<table>
<thead>
<tr class="header">
<th align="right">Número.de.lanzamientos</th>
<th align="right">Número.de.caras</th>
<th align="right">Proporción</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">0.50</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">2</td>
<td align="right">0.67</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">0.75</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
<td align="right">0.80</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">4</td>
<td align="right">0.67</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">4</td>
<td align="right">0.57</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">5</td>
<td align="right">0.63</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">6</td>
<td align="right">0.67</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">7</td>
<td align="right">0.70</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="right">8</td>
<td align="right">0.73</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">8</td>
<td align="right">0.67</td>
</tr>
<tr class="odd">
<td align="right">13</td>
<td align="right">9</td>
<td align="right">0.69</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">10</td>
<td align="right">0.71</td>
</tr>
<tr class="odd">
<td align="right">15</td>
<td align="right">10</td>
<td align="right">0.67</td>
</tr>
<tr class="even">
<td align="right">16</td>
<td align="right">10</td>
<td align="right">0.63</td>
</tr>
<tr class="odd">
<td align="right">17</td>
<td align="right">10</td>
<td align="right">0.59</td>
</tr>
<tr class="even">
<td align="right">18</td>
<td align="right">10</td>
<td align="right">0.56</td>
</tr>
<tr class="odd">
<td align="right">19</td>
<td align="right">10</td>
<td align="right">0.53</td>
</tr>
<tr class="even">
<td align="right">20</td>
<td align="right">11</td>
<td align="right">0.55</td>
</tr>
</tbody>
</table>
<p>Tengamos en cuenta que al comienzo de esta secuencia, la <em>proporción</em> de caras fluctúa enormemente, comenzando en .00 y subiendo tan alto como .80. Conforme aumenta el número de lanzamientos, uno tiene la impresión de que este efecto se amortigua un poco, mientras que los valores se aproximan cada vez más a la respuesta “correcta” de .50. Esta es la definición frecuentista de probabilidad en pocas palabras: lanzar una moneda justa una y otra vez, y a medida que <span class="math inline">\(N\)</span> crece (se acerca al infinito, denotado como <span class="math inline">\(N\rightarrow \infty\)</span>), la proporción de caras convergerá en el 50%. Tecnicismos matemáticos aparte, cualitativamente hablando, es así es como los frecuentistas definen la probabilidad. Desafortunadamente, no tengo un número infinito de monedas, o la paciencia infinita requerida para lanzar una moneda un número infinito de veces. Sin embargo, existen los ordenadores, y los ordenadores se destacan por la ejecución repetitiva de tareas sin sentido como esta. Entonces, al simular 1.000 lanzamientos de moneda y repetir este procesos 4 veces (para darle solidez a los resultados), podemos ver qué sucede con la proporción <span class="math inline">\(N_C / N\)</span> a medida que <span class="math inline">\(N\)</span> aumenta. Los resultados se muestran en la Figura <a href="probability.html#fig:frequentistprobability">1.1</a> aunque también puedes hacer tú la simulación haciendo click <a href="https://leudave.shinyapps.io/cara_cruz/">aquí</a>. Podemos apreciar que la <em>proporción de caras observadas</em> deja de fluctuar conforme aumenta el número de lanzamientos; cuando lo hace, el número que finalmente obtenemos es el verdadera probabilidad de salga cara.</p>
<div class="figure"><span id="fig:frequentistprobability"></span>
<img src="FdI2_files/figure-html/frequentistprobability-1.png" alt=" Una imagen de cómo funciona la probabilidad frecuentista. Si lanzas una moneda justa una y otra vez, la proporción de caras deja de fluctuar y converge hacia la probabilidad real de 0.5. Cada panel muestra uno de las cuatro simulaciones con 1.000 lanzamientos cada uno. Aunque ninguna de estas simulaciones terminó con un valor exacto de .5, si hubiéramos extendido el experimento por un número infinito de lanzamientos lo habríamos conseguido." width="672" />
<p class="caption">
Figure 1.1:  Una imagen de cómo funciona la probabilidad frecuentista. Si lanzas una moneda justa una y otra vez, la proporción de caras deja de fluctuar y converge hacia la probabilidad real de 0.5. Cada panel muestra uno de las cuatro simulaciones con 1.000 lanzamientos cada uno. Aunque ninguna de estas simulaciones terminó con un valor exacto de .5, si hubiéramos extendido el experimento por un número infinito de lanzamientos lo habríamos conseguido.
</p>
</div>
<p>La definición frecuentista de probabilidad tiene algunas características que le hacen deseable. En primer lugar, es objetivo: la probabilidad de un evento se basa <em>necesariamente</em> en el mundo real. La única forma en que declaraciones de probabilidad puedan tener sentido es si se refieren a (una secuencia de) eventos que ocurren en el universo físico. En segundo lugar, es inequívoco: dos personas que miran como se desarrolla la misma secuencia de eventos, al tratar de calcular la probabilidad de un evento, inevitablemente deberán llegar a la misma respuesta. Sin embargo, también tiene algunas características no tan deseables. En primer lugar, no existen secuencias infinitas en el mundo físico. Supongamos que has encontrado una moneda en el suelo y has comenzado a lanzarla varias veces. Cada vez que aterriza, impacta en el suelo. Cada impacto daña un poco la moneda; eventualmente, la moneda será inutilizable. Entonces, uno podría preguntarse si realmente tiene sentido fingir que una secuencia “infinita” de lanzamientos de monedas es un concepto significativo u objetivo. No podemos decir que una “secuencia infinita” de eventos es algo real en el universo físico, porque el universo físico no permite nada infinito. Así, podemos ver que la definición frecuentista tiene un alcance limitado. Hay muchas cosas por ahí a las que los seres humanos estamos felices de asignar probabilidades en el día a día, pero que no pueden (ni siquiera en teoría) ser planteadas como una secuencia hipotética de eventos. Por ejemplo, si un meteorólogo aparece en televisión y dice: “la probabilidad de lluvia en Pamplona el 2 de noviembre del 2048 es del 60%” nosotros podemos aceptarlo sin rechistar. Pero desde un punto de vista frecuentista, no queda tan claro cómo podemos definirlo. Sólo hay una ciudad de Pamplona (en Navarra), y sólo un 2 de noviembre del 2048. Aquí no hay una secuencia infinita de eventos, solo una cosa de una vez. La probabilidad frecuentista nos <em>prohíbe</em> hacer declaraciones de probabilidad sobre un solo evento. Desde la perspectiva frecuentista, lloverá mañana o o no lloverá; no hay una “probabilidad” que se pueda adjuntar a un sólo evento no repetible. Sin embargo, existen algunos trucos que los frecuentistas pueden utilizar para solucionar esta situación. Una posibilidad es que el meteorólogo en realidad nos quiera decir algo así: “Existe una categoría de días para la que predigo un 60% probabilidad de lluvia; si miramos sólo esos días para los que hago esta predicción, entonces en el 60% de esos días lloverá realmente”. Es un poco extraño y contradictorio pensarlo de esta manera, pero los frecuentistas hacen esto a veces.</p>
</div>
<div id="la-vision-bayesiana" class="section level3">
<h3><span class="header-section-number">1.2.2</span> La visión bayesiana</h3>
<p>La alternativa a la visión frecuentista, la <strong><em>visión bayesiana</em></strong> de la probabilidad, a menudo se denomina visión subjetivista, y es una visión relativamente minoritaria entre los estadísticos, aunque ha ido ganando terreno constantemente a lo largo de las últimas décadas. Hay muchos formas de bayesianismo, lo que hace difícil decir exactamente cuál es “la” visión bayesiana. La forma más fácil de entender la probabilidad subjetiva es al definir la probabilidad de un evento como el <strong><em>grado de creencia</em></strong> que un agente inteligente y racional asigna a la verdad de ese evento. Desde esa perspectiva, las probabilidades no existen en el mundo, sino más bien en los pensamientos y suposiciones de las personas y otros seres inteligentes. Sin embargo, para que este enfoque funcione, necesitamos una forma de operacionalizar este “grado de creencia”. Una forma de hacerlo es formalizándolo en términos de una “apuesta racional” aunque existen muchas otras formas. Supongamos que creo que existe una probabilidad de que llueva mañana de un 60%. Si alguien me hace una apuesta, si llueve mañana, gano $5, pero si no llueve, pierdo $5. Claramente, desde mi punto de vista, esta es una buena apuesta. Por otro lado, si creo que la probabilidad de lluvia es sólo del 40%, entonces es una mala apuesta. Por lo tanto, podemos poner en práctica la noción de una “probabilidad subjetiva” en términos de qué apuestas que estoy dispuesto a aceptar.</p>
<p>¿Cuáles son las ventajas y desventajas del enfoque bayesiano? La principal ventaja es que le permite asignar probabilidades a cualquier evento. No esta limitado a aquellos eventos que son repetibles. La principal desventaja (para muchas personas) es que no podemos ser realmente objetivos - especificar una probabilidad requiere que especifiquemos la entidad que tiene el grado de creencia que estamos examinando. Esta entidad puede ser un humano, un extraterrestre, un robot o incluso un estadístico, pero tiene que ser un agente inteligente que sea capaz de creer en cosas. Para muchas personas esto representa un inconveniente: parece hacer que la probabilidad sea arbitraria. Si bien el enfoque bayesiano requiere que el agente en cuestión sea racional (es decir, que obedezca las reglas de la probabilidad), permite que todos tengan sus propias creencias; yo puedo creer que la moneda es justa mientras que otro no, aunque ambos seamos racionales. La visión frecuentista no permite que dos observadores atribuyan diferentes probabilidades al mismo evento: cuando eso sucede, al menos uno de ellos debe estar equivocado. La visión bayesiana no evita que esto ocurra. Dos observadores con diferentes conocimientos previos pueden tener creencias diferentes sobre el mismo evento. En otras palabras, mientras que la visión frecuentista se puede considerar como demasiado estrecha (prohíbe muchas cosas a las cuales queremos asignar probabilidades), la visión bayesiana puede resultar demasiado amplia (permite demasiadas diferencias entre observadores).</p>
</div>
<div id="cual-es-la-diferencia-y-quien-tiene-razon" class="section level3">
<h3><span class="header-section-number">1.2.3</span> ¿Cuál es la diferencia? ¿Y quién tiene razón?</h3>
<p>Ahora que hemos visto ambas visiones estadísticas de forma independiente, es necesario compararlas. Regresemos al hipotético juego de fútbol de robots que comentamos comienzo del tema. ¿Que dirían un frecuentista y un bayesiano sobre las tres afirmaciones? ¿Qué enunciado sería la definición de probabilidad correcta para el frecuentista? ¿Y para el bayesiano? ¿Es posible que alguno de los enunciados no tenga sentido para cualquiera de los dos? Si entendemos ambas perspectivas, podemos intuir cómo responder a estas preguntas.</p>
<p>Entendiendo las diferencias, podemos preguntarnos a continuación cuál de dos enfoques es el <em>correcto</em>. Sin embargo, no existe una respuesta correcta. Matemáticamente hablando, no hay nada incorrecto sobre la forma en que los frecuentistas piensan sobre secuencias de eventos, ni hay nada incorrecto acerca de la forma en que los bayesianos definen las creencias de un agente racional. De hecho, si vamos al detalle, los bayesianos y los frecuentistas en realidad están de acuerdo en muchas cosas. Muchos métodos frecuentistas conducen a decisiones que los bayesianos pensarían que toma un agente racional. Muchos métodos bayesianos tienen buenas propiedades frecuentistas.</p>
<p>En cualquier caso, la mayor parte de los métodos y análisis estadísticos en la literatura se basan en el enfoque frecuentista. Por lo tanto, el objetivo de esta asignatura es cubrir aproximadamente el mismo temario que una clase típica de estadística de pregrado en ciencias de la educación, y si queremos entender las herramientas estadísticas utilizadas por la mayoría de los educadores en investigación, necesitaremos una buena comprensión de los métodos frecuentistas.</p>
</div>
</div>
<div id="basicprobability" class="section level2">
<h2><span class="header-section-number">1.3</span> Teoría de probabilidad básica</h2>
<p>A pesar de los argumentos ideológicos entre bayesianos y frecuentistas, existe un consenso más o menos generalizado sobre las reglas que la probabilidad debe obedecer. Hay muchas formas de abordar estas reglas. El enfoque más utilizado se basa en el trabajo de Andrey Kolmogorov, uno de los grandes matemáticos soviéticos del siglo XX. No entraremos mucho en detalle, pero aprenderemos en qué consisten y cómo utilizarlas a través del siguiente ejemplo.</p>
<div id="introduccion-a-las-distribuciones-de-probabilidad" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Introducción a las distribuciones de probabilidad</h3>
<p>Un hecho comprobado sobre mi vida es que sólo tengo 5 pares de pantalones: tres pares de vaqueros, los pantalones de un traje y un par de pantalones de chándal. Lo más triste es que les he dado nombres: los llamo <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, <span class="math inline">\(X_4\)</span> y <span class="math inline">\(X_5\)</span>. Diariamente, por la mañana, elijo un único par de esos pantalones que voy a usar. Si yo tuviera que describir esta situación usando el lenguaje de la teoría de la probabilidad, me referiría a cada par de pantalones (es decir, a cada <span class="math inline">\(X\)</span>) como un <strong><em>evento elemental</em></strong>. La característica clave de estos eventos elementales es que cada vez que hacemos una observación (por ejemplo, cada vez que escojo un par de pantalones), el resultado será uno y solo uno de estos eventos. Como he dicho antes, siempre uso exactamente sólo un par de pantalones, así que mis pantalones cumplen con esta restricción. Del mismo modo, al conjunto de todos los eventos posibles se le denomina <strong><em>espacio muestral</em></strong>. Siguiendo con el ejemplo, mi espacio muestral sería el armario que contiene los 5 pantalones.</p>
<p>Bien, ahora que tenemos un espacio muestral (un armario), que está construido a partir de muchas posibles eventos elementales (pantalones), lo que queremos hacer es asignar una <strong><em>probabilidad</em></strong> a cada uno de estos eventos elementales. Para un evento <span class="math inline">\(X\)</span>, la probabilidad de ese evento <span class="math inline">\(P(X)\)</span> es un número que se encuentra entre 0 y 1. Cuanto mayor sea el valor de <span class="math inline">\(P(X)\)</span>, más probable será que ocurra el evento. Entonces, por ejemplo, si <span class="math inline">\(P(X) = 0\)</span>, significa que el evento <span class="math inline">\(X\)</span> es imposible (es decir, nunca uso esos pantalones). Por otro lado, si <span class="math inline">\(P(X) = 1\)</span> significa que el evento <span class="math inline">\(X\)</span> es seguro que ocurra (es decir, siempre uso esos pantalones). Los valores de probabilidad intermedios, significan que a veces uso esos pantalones (y a veces no). Por ejemplo, una <span class="math inline">\(P(X) = 0.5\)</span> significa que uso esos pantalones la mitad de las veces.</p>
<p>Llegados a este punto, lo siguiente que debemos entender es que “algo siempre sucede”. Cada vez que me pongo unos pantalones, realmente termino usando esos pantalones. Lo que esto significa en términos probabilísticos, es que las probabilidades de todos los eventos elementales siempre suman 1. Esto se conoce como la <strong><em>ley de probabilidad total</em></strong>. Si se cumplen estos requisitos (tenemos número <span class="math inline">\(X\)</span> de pantalones, cada par con una probabilidad <span class="math inline">\(P(X)\)</span> de usarlos que en total suman 1), entonces lo que tenemos es una <strong><em>distribución de probabilidad</em></strong>.Veamos un ejemplo de distribución de probabilidad</p>
<table>
<thead>
<tr class="header">
<th align="left">Pantalones</th>
<th align="left">V..azules</th>
<th align="left">V..grises</th>
<th align="left">V..negros</th>
<th align="left">Traje.negro</th>
<th align="left">Chándal.azul</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Nombre</td>
<td align="left"><span class="math inline">\(X_1\)</span></td>
<td align="left"><span class="math inline">\(X_2\)</span></td>
<td align="left"><span class="math inline">\(X_3\)</span></td>
<td align="left"><span class="math inline">\(X_4\)</span></td>
<td align="left"><span class="math inline">\(X_5\)</span></td>
</tr>
<tr class="even">
<td align="left">Probabilidad</td>
<td align="left"><span class="math inline">\(P(X_1) = .5\)</span></td>
<td align="left"><span class="math inline">\(P(X_2) = .3\)</span></td>
<td align="left"><span class="math inline">\(P(X_3) = .1\)</span></td>
<td align="left"><span class="math inline">\(P(X_4) = 0\)</span></td>
<td align="left"><span class="math inline">\(P(X_5) = .1\)</span></td>
</tr>
</tbody>
</table>
<p>Cada uno de estos eventos tiene una probabilidad que se encuentra entre 0 y 1, y si sumamos la probabilidad de todos eventos, suman 1. Incluso podemos dibujar un gráfico de barras para visualizar esta distribución, como se muestra en la Figura <a href="probability.html#fig:pantsprob">1.2</a>.</p>
<div class="figure"><span id="fig:pantsprob"></span>
<img src="FdI2_files/figure-html/pantsprob-1.png" alt="Demostración visual de la distribución de probabilidad de los &quot;pantalones&quot;. Existen 5 &quot;eventos elementales&quot;, que se corresponden con mis 5 pares de pantalones. Cada evento tiene una probabilidad de ocurrir: esta probabilidad es un número entre 0 y 1. La suma de estas probabilidades es 1." width="672" />
<p class="caption">
Figure 1.2: Demostración visual de la distribución de probabilidad de los “pantalones”. Existen 5 “eventos elementales”, que se corresponden con mis 5 pares de pantalones. Cada evento tiene una probabilidad de ocurrir: esta probabilidad es un número entre 0 y 1. La suma de estas probabilidades es 1.
</p>
</div>
<p>Es importante señalar que la teoría de probabilidades permite hablar acerca de eventos elementales pero también sobre los <strong><em>eventos no elementales</em></strong>. La forma más fácil de ilustrar este concepto es con un ejemplo. Siguiendo con el ejemplo de los pantalones, es perfectamente posible hablar sobre la probabilidad de usar vaqueros. Bajo esta premisa, podemos decir que el evento “yo uso vaqueros” es posible siempre y cuando ocurra alguno de los eventos elementales apropiados; en este caso “vaqueros azules”, “vaqueros negros” o “vaqueros grises”. En términos matemáticos, definimos al evento <span class="math inline">\(E\)</span> “vaqueros” como el conjunto de eventos elementales <span class="math inline">\((X_1, X_2, X_3)\)</span>. Si se produce alguno de estos eventos elementales, también podemos decir que <span class="math inline">\(E\)</span> ha ocurrido. Por lo tanto, podemos decir que la probabilidad <span class="math inline">\(P(E)\)</span> es simplemente la suma de esos tres eventos, así <span class="math display">\[
P(E) = P(X_1) + P(X_2) + P(X_3)
\]</span> y, dado que las probabilidades de los vaqueros azules, grises y negros son respectivamente .5, .3 y .1, la probabilidad total de usar vaqueros es igual a .9.</p>
<p>Todo esto parece obvio y simple. Sin embargo, a partir de estos simples comienzos, es posible construir algunas herramientas matemáticas más complejas y poderosas. En la siguiente Tabla se muestran algunas de las otras reglas que deben de cumplirse para poder calcular probabilidades.</p>
<table>
<thead>
<tr class="header">
<th align="left">Castellano</th>
<th align="left">Notacion</th>
<th align="left">Igual</th>
<th align="left">Formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">No <span class="math inline">\(A\)</span></td>
<td align="left"><span class="math inline">\(P(\neg A)\)</span></td>
<td align="left">=</td>
<td align="left"><span class="math inline">\(1-P(A)\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(A\)</span> o <span class="math inline">\(B\)</span></td>
<td align="left"><span class="math inline">\(P(A \cup B)\)</span></td>
<td align="left">=</td>
<td align="left"><span class="math inline">\(P(A) + P(B) - P(A \cap B)\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span></td>
<td align="left"><span class="math inline">\(P(A \cap B)\)</span></td>
<td align="left">=</td>
<td align="left"><span class="math inline">\(P(A \vert B) P(B)\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="binomial" class="section level2">
<h2><span class="header-section-number">1.4</span> La distribución binomial</h2>
<p>Como hemos visto, las distribuciones de probabilidad pueden variar enormemente, por lo que existe un gran número de distribuciones posibles. Sin embargo, no todas son igual de importantes. Las distribuciones más importantes, y de las que hablaremos en esta asignatura son cinco: la distribución binomial, la distribución normal, la distribución <span class="math inline">\(t\)</span>, la distribución <span class="math inline">\(\chi^2\)</span> (“chi-cuadrada”) y la distribución <span class="math inline">\(F\)</span>. Daremos una breve introducción a las cinco, prestando especial atención a las distribuciones binomial y normal. Comenzaremos con la distribución más simple de las cinco, la distribución binomial.</p>
<div id="introduccion-al-binomio" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Introducción al binomio</h3>
<p>La teoría de la probabilidad se creó originalmente para intentar describir cómo funcionaban los juegos de azar, por lo que parece adecuado que nuestra discusión sobre la <strong><em>distribución binomial</em></strong> involucre una discusión sobre tirar dados y lanzar monedas. Imaginemos el siguiente “experimento”: tengo en mi poder 20 dados iguales de seis caras. En una de las caras de cada dado hay una imagen de una calavera; las otras cinco caras están en blanco. Si tiro los 20 dados, ¿cuál es la probabilidad de que obtenga exactamente 4 calaveras? Asumiendo que los dados son justos, sabemos que la probabilidad de que en un dado salga la calavera es de de 1 en 6; dicho de otra forma, la probabilidad de que salga calavera en un solo dado es de aproximadamente <span class="math inline">\(.167\)</span>. Esta información es suficiente para poder responder a nuestra pregunta anterior, así que veamos cómo hacerlo.</p>
<p>Primero, pondremos algunos nombres a los eventos con su respectiva notación. Dejaremos que <span class="math inline">\(N\)</span> denote el número de dados que se tiran en nuestro experimento; a esto se le conoce como el <strong><em>parámetro de tamaño</em></strong> de nuestra distribución binomial. También usaremos <span class="math inline">\(\theta\)</span> para referirnos a la probabilidad de que al tirar un solo dado salga calavera, una cantidad que generalmente se denomina como la <strong><em>probabilidad de éxito</em></strong> del binomio.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Finalmente, usaremos <span class="math inline">\(X\)</span> para referirnos a los resultados de nuestro experimento, es decir, la cantidad de calaveras que obtengo cuando lanzo los dados. Dado que el valor real de <span class="math inline">\(X\)</span> se debe al azar, nos podemos referir a ella como una <strong><em>variable aleatoria</em></strong>. En cualquier caso, ahora que tenemos toda esta terminología y notación, podemos utilizarlos para exponer el problema que planteábamos en el párrafo anterior con mayor precisión. La cantidad que queremos calcular es la probabilidad de que <span class="math inline">\(X = 4\)</span> sabiendo que <span class="math inline">\(\theta = .167\)</span> y <span class="math inline">\(N=20\)</span>. La “forma” general de esta probabilidad que me interesa calcular podría escribirse como, <span class="math display">\[
  P(X \ | \ \theta, N)
\]</span> donde estamos interesados en el caso específico donde <span class="math inline">\(X=4\)</span>, <span class="math inline">\(\theta = .167\)</span> y <span class="math inline">\(N=20\)</span>. Hace falta un elemento de notación más antes de continuar con la solución del problema. Si yo quiero decir que <span class="math inline">\(X\)</span> se genera aleatoriamente a partir de una distribución binomial con los parámetros <span class="math inline">\(\theta\)</span> y <span class="math inline">\(N\)</span>, la notación que usaría para expresarlo sería la siguiente: <span class="math display">\[
  X \sim \mbox{Binomial}(\theta, N)
\]</span></p>
<p>Aunque no utilizaremos las fórmulas para hacer cálculos formalmente, dejaré la fórmula de la distribución binomial en la Tabla <a href="probability.html#tab:distformulas">1.1</a>, ya que puede ser útil si se quieren entender temas más avanzados con algo de profundidad. Por lo pronto, analizaremos como se ve una distribución binomial (puedes hacerlo tú mismo si entras <a href="https://leudave.shinyapps.io/distribuciones/">aquí</a>). La Figura <a href="probability.html#fig:binomial1">1.3</a> dibuja la probabilidad binomial para todos los valores posibles de <span class="math inline">\(X\)</span> de nuestro experimento de lanzamiento de dados, partiendo desde <span class="math inline">\(X=0\)</span> (ninguna sale calavera) hasta <span class="math inline">\(X=20\)</span> (salen todas calaveras). Hay que tener en cuenta que esto es básicamente un gráfico de barras, al igual que la gráfica de la “probabilidad de pantalones” de la Figura <a href="probability.html#fig:pantsprob">1.2</a>. En el eje horizontal tenemos todos los eventos posibles, y en el eje vertical podemos leer la probabilidad de que ocurra cada uno de esos eventos. Por lo tanto, la probabilidad de que salgan 4 calaveras al tirar 20 dados es de aproximadamente 0,20 (la respuesta exacta es 0,2022036, como veremos en un momento). En otras palabras, esperaría que ese evento suceda aproximadamente el 20% de las veces que se lleve a cabo este experimento.</p>
<table>
<caption><span id="tab:distformulas">Table 1.1: </span>Fórmulas para las distribuciones binomial y normal. En la ecuación de la binomial, <span class="math inline">\(X!\)</span> es una función factorial (es decir, multiplica todos los números enteros de 1 hasta <span class="math inline">\(X\)</span>), y en la de la distribución normal “exp” se refiere a una función exponencial.</caption>
<thead>
<tr class="header">
<th align="left">Binomial</th>
<th align="left">Normal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(P(X | \theta, N) = \displaystyle\frac{N!}{X! (N-X)!} \theta^X (1-\theta)^{N-X}\)</span></td>
<td align="left"><span class="math inline">\(p(X | \mu, \sigma) = \displaystyle\frac{1}{\sqrt{2\pi}\sigma} \exp \left( -\frac{(X - \mu)^2}{2\sigma^2} \right)\)</span></td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:binomial1"></span>
<img src="FdI2_files/figure-html/binomial1-1.png" alt=" La distribución binomial con parámetro de tamaño de $N=20$ y una probabilidad de éxito de $theta = 1/6$. Cada barra vertical representa la probabilidad de un resultado específico (un valor posible de $X$). Ya que esta es una distribución de probabilidad, cada una de las probabilidades debe ser un número entre 0 y 1, y la altura de las barras también deben sumar 1." width="672" />
<p class="caption">
Figure 1.3:  La distribución binomial con parámetro de tamaño de <span class="math inline">\(N=20\)</span> y una probabilidad de éxito de <span class="math inline">\(theta = 1/6\)</span>. Cada barra vertical representa la probabilidad de un resultado específico (un valor posible de <span class="math inline">\(X\)</span>). Ya que esta es una distribución de probabilidad, cada una de las probabilidades debe ser un número entre 0 y 1, y la altura de las barras también deben sumar 1.
</p>
</div>
<p>Para darte una idea de cómo cambia la distribución binomial cuando modificamos los valores de <span class="math inline">\(\theta\)</span> y <span class="math inline">\(N\)</span>, supongamos que en lugar de tirar dados, en realidad estoy lanzando monedas. Esta vez, mi experimento implica lanzar una moneda justa repetidamente, y el resultado que me interesa es la cantidad de caras que observo. En este escenario, la probabilidad de éxito ahora es de <span class="math inline">\(\theta = 1/2\)</span>. Supongamos que tirara la moneda <span class="math inline">\(N=20\)</span> veces. En este ejemplo, he cambiado la probabilidad de éxito, pero mantuve el tamaño de la muestra del experimento. ¿Qué efecto tiene este cambio en nuestra distribución binomial? Bueno, como la Figura <a href="probability.html#fig:binomial2a">1.4</a> muestra, el efecto principal fue el desplazamiento de toda la distribución hacia la derecha, como era de esperar. ¿Y si lanzamos una moneda <span class="math inline">\(N=100\)</span> veces? En este caso obtendremos algo como lo de la Figura <a href="probability.html#fig:binomial2b">1.5</a>. La distribución se mantiene aproximadamente en el medio, pero hay un poco más de variabilidad en los posibles resultados.</p>
<div class="figure"><span id="fig:binomial2a"></span>
<img src="FdI2_files/figure-html/binomial2a-1.png" alt="Dos distribuciones binomiales, que involucran un escenario en el que lanzo una moneda justa, donde la probabilidad de éxito es $theta = 1/2$. Asumimos que estoy lanzando la moneda $N=20$ veces." width="672" />
<p class="caption">
Figure 1.4: Dos distribuciones binomiales, que involucran un escenario en el que lanzo una moneda justa, donde la probabilidad de éxito es <span class="math inline">\(theta = 1/2\)</span>. Asumimos que estoy lanzando la moneda <span class="math inline">\(N=20\)</span> veces.
</p>
</div>
<div class="figure"><span id="fig:binomial2b"></span>
<img src="FdI2_files/figure-html/binomial2b-1.png" alt="Dos distribuciones binomiales, que involucran un escenario en el lanzo una moneda justa, donde la probabilidad de éxito subyacente es $theta = 1/2$. Asumimos que estoy lanzando la moneda $N=100$ veces." width="672" />
<p class="caption">
Figure 1.5: Dos distribuciones binomiales, que involucran un escenario en el lanzo una moneda justa, donde la probabilidad de éxito subyacente es <span class="math inline">\(theta = 1/2\)</span>. Asumimos que estoy lanzando la moneda <span class="math inline">\(N=100\)</span> veces.
</p>
</div>
</div>
</div>
<div id="normal" class="section level2">
<h2><span class="header-section-number">1.5</span> La distribución normal</h2>
<p>Si bien la distribución binomial es conceptualmente la distribución más sencilla de entender, no es la más importante. Ese honor le corresponde a la <strong><em>distribución normal</em></strong>, también conocida como “curva de campana” o como “distribución gaussiana” o “campana de Gauss”. Una distribución normal se describe utilizando dos parámetros, la media de la distribución <span class="math inline">\(\mu\)</span> y la desviación estándar de la distribución <span class="math inline">\(\sigma\)</span>. La notación que utilizamos para decir que una variable <span class="math inline">\(X\)</span> se distribuye normalmente es la siguiente:</p>
<p><span class="math display">\[
  X \sim \mbox{Normal}(\mu,\sigma)
\]</span> Al igual que con la distribución binomial, he incluido la fórmula para la distribución normal en la tabla <a href="probability.html#tab:distformulas">1.1</a>, porque creo que es lo suficientemente importante como para que todos los que aprenden algo de estadística al menos la conozcan, aunque no nos enfoquemos en ella.</p>
<p>Vamos intentar descifrar lo que significa que una variable esté normalmente distribuida. Echemos un vistazo a la Figura <a href="probability.html#fig:normdist">1.6</a>, que muestra una distribución normal con media <span class="math inline">\(\mu = 0\)</span> y desviación estándar <span class="math inline">\(\sigma = 1\)</span>. Con un poco de imaginación, podemos apreciar de dónde viene el nombre “curva de campana”. A diferencia de los gráficos sobre la distribución binomial, la imagen de la distribución normal en la Figura <a href="probability.html#fig:normdist">1.6</a> muestra una curva suave en lugar de barras “tipo histograma”. Esto no es arbitrario: la distribución normal es continua, mientras que la distribución binomial es discreta. Por ejemplo, en el experimento de tiro de dados de la sección anterior, es posible obtener 3 calaveras o 4 calaveras, mientras que un valor intermedio como 3.9 es imposible de obtener. Este hecho se ve reflejado en la Figura <a href="probability.html#fig:binomial1">1.3</a>, donde tenemos una barra ubicada en <span class="math inline">\(X=3\)</span> y otra en <span class="math inline">\(X=4\)</span>, pero entre ellas no hay nada. En cambio, los valores continuos no tienen esta restricción. Por ejemplo, supongamos que estamos hablando del tiempo. La temperatura de un día primavera podría ser de 23 grados, 24 grados, 23.9 grados o cualquier cosa intermedia, ya que la temperatura es una variable continua, por lo que una distribución normal podría ser la herramienta apropiada para describir las diferentes temperaturas en los días de primavera.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<div class="figure"><span id="fig:normdist"></span>
<img src="FdI2_files/figure-html/normdist-1.png" alt=" Distribución normal con media $mu = 0$ y desviación estándar $sigma = 1$. El eje $x$ corresponde con el valor de alguna variable, y el eje $y$ nos dice qué tan probable es que observemos ese valor. Sin embargo, vemos como el eje $y$ se denomina &quot;Densidad de Probabilidad&quot; y no &quot;Probabilidad&quot;. La altura de la curva no representa como tal la probabilidad de observar un valor particular de $x$. Sin embargo, las alturas nos informan sobre qué valores de $x$ son más probables (¡los más altos!)." width="672" />
<p class="caption">
Figure 1.6:  Distribución normal con media <span class="math inline">\(mu = 0\)</span> y desviación estándar <span class="math inline">\(sigma = 1\)</span>. El eje <span class="math inline">\(x\)</span> corresponde con el valor de alguna variable, y el eje <span class="math inline">\(y\)</span> nos dice qué tan probable es que observemos ese valor. Sin embargo, vemos como el eje <span class="math inline">\(y\)</span> se denomina “Densidad de Probabilidad” y no “Probabilidad”. La altura de la curva no representa como tal la probabilidad de observar un valor particular de <span class="math inline">\(x\)</span>. Sin embargo, las alturas nos informan sobre qué valores de <span class="math inline">\(x\)</span> son más probables (¡los más altos!).
</p>
</div>
<p>Una vez visto esto, vamos a analizar cómo funciona una distribución normal. En primer lugar, veamos qué es lo que sucede cuando jugamos con los parámetros de la distribución (puedes hacerlo tú mismo si entras en este enlace). La Figura <a href="probability.html#fig:normmean">1.7</a> muestra distribuciones normales que tienen medias diferentes, pero con la misma desviación estándar. Como es de esperar, todas estas distribuciones tienen la misma “anchura”. La unica diferencia entre ellas es que se han desplazado hacia la izquierda o hacia la derecha. En todos los demás aspectos son idénticas. Por el contrario, si aumentamos la desviación estándar mientras mantenemos la media constante, el pico de la distribución permanece en el mismo lugar, pero la distribución se amplía, como podemos ver en la Figura <a href="probability.html#fig:normsd">1.8</a>. Sin embargo, cuando ampliamos la distribución, la altura del pico disminuye. Esto <em>tiene</em> que suceder: de la misma forma que las alturas de las barras de una distribución binomial discreta tienen que <em>sumar</em> 1, el total del <em>área bajo la curva</em> de una distribución normal debe ser igual a 1.</p>
<div class="figure"><span id="fig:normmean"></span>
<img src="FdI2_files/figure-html/normmean-1.png" alt="Gráfica que demuestra lo que sucede cuando se cambia la media de una distribución normal. La línea sólida representa una distribución normal con media de $mu=4$.  La línea discontinua muestra una distribución normal con una media de $mu=7$.  En ambos casos, la desviación estándar es de $sigma=1$. Vemos como las dos distribuciones tienen la misma forma, pero la distribución con la línea discontinua se desplaza hacia la derecha." width="672" />
<p class="caption">
Figure 1.7: Gráfica que demuestra lo que sucede cuando se cambia la media de una distribución normal. La línea sólida representa una distribución normal con media de <span class="math inline">\(mu=4\)</span>. La línea discontinua muestra una distribución normal con una media de <span class="math inline">\(mu=7\)</span>. En ambos casos, la desviación estándar es de <span class="math inline">\(sigma=1\)</span>. Vemos como las dos distribuciones tienen la misma forma, pero la distribución con la línea discontinua se desplaza hacia la derecha.
</p>
</div>
<div class="figure"><span id="fig:normsd"></span>
<img src="FdI2_files/figure-html/normsd-1.png" alt="Una ilustración de lo que sucede cuando cambia la desviación estándar de una distribución normal. Ambas distribuciones tienen una media de $mu=5$, pero diferentes desviaciones estándar. La línea continua dibuja una distribución con una desviación estándar $sigma=1$, y la línea discontinua muestra una distribución con desviación estándar de $sigma=2$. Como consecuencia, ambas distribuciones están centradas en el mismo lugar, pero la distribución con la línea discontinua es más ancha que la otra." width="672" />
<p class="caption">
Figure 1.8: Una ilustración de lo que sucede cuando cambia la desviación estándar de una distribución normal. Ambas distribuciones tienen una media de <span class="math inline">\(mu=5\)</span>, pero diferentes desviaciones estándar. La línea continua dibuja una distribución con una desviación estándar <span class="math inline">\(sigma=1\)</span>, y la línea discontinua muestra una distribución con desviación estándar de <span class="math inline">\(sigma=2\)</span>. Como consecuencia, ambas distribuciones están centradas en el mismo lugar, pero la distribución con la línea discontinua es más ancha que la otra.
</p>
</div>
<p>Antes de seguir adelante, quiero señalar una característica importante de la distribución normal. Independientemente de los valores de la media y la desviación estándar, un 68.3% del área de la curva cae dentro de 1 desviación estándar sobre la media. Del mismo modo, el 95.4% de la distribución cae dentro de 2 desviaciones estándar sobre la media, y el 99.7% de la distribución está dentro de 3 desviaciones estándar. Esta idea se ilustra en las Figuras <a href="probability.html#fig:sdnorm1">1.9</a> y <a href="probability.html#fig:sdnorm2">1.10</a>.</p>
<div class="figure"><span id="fig:sdnorm1"></span>
<img src="FdI2_files/figure-html/sdnorm1-1.png" alt="El área bajo la curva indica la probabilidad de que una observación se encuentre dentro de un rango determinado. Las línea continua traza una distribución normal con media $mu=0$ y desviación estándar $sigma=1$. El área sombreada ilustra el 'área bajo la curva' para dos casos importantes. En el panel a, podemos ver que hay es un  68.3% de probabilidad de que una observación caiga dentro de 1 desviación estándar sobre la media. En el panel b, vemos que existe una probabilidad del 95.4% de que una observación se encuentre dentro de 2 desviaciones estándar sobre la media." width="672" />
<p class="caption">
Figure 1.9: El área bajo la curva indica la probabilidad de que una observación se encuentre dentro de un rango determinado. Las línea continua traza una distribución normal con media <span class="math inline">\(mu=0\)</span> y desviación estándar <span class="math inline">\(sigma=1\)</span>. El área sombreada ilustra el ‘área bajo la curva’ para dos casos importantes. En el panel a, podemos ver que hay es un 68.3% de probabilidad de que una observación caiga dentro de 1 desviación estándar sobre la media. En el panel b, vemos que existe una probabilidad del 95.4% de que una observación se encuentre dentro de 2 desviaciones estándar sobre la media.
</p>
</div>
<div class="figure"><span id="fig:sdnorm2"></span>
<img src="FdI2_files/figure-html/sdnorm2-1.png" alt="Dos ejemplos más sobre el concepto del 'área bajo la curva'. Existe un 15.9% de probabilidad de que una observación se encuentre 1 desviación estándar o menos por debajo de la media (panel a), y una probabilidad del 34.1% de que una observación sea mayor que una desviación estándar por debajo de la media pero menor que la media (panel b). Si sumamos estos dos valores, obtendremos 15.9% + 34.1% = 50%. Para datos que estén normalmente distribuidos, existe un 50% de probabilidad de que una observación caiga por debajo de la media. Esto implica que existe un 50% de probabilidad de que caiga por encima de la media." width="672" />
<p class="caption">
Figure 1.10: Dos ejemplos más sobre el concepto del ‘área bajo la curva’. Existe un 15.9% de probabilidad de que una observación se encuentre 1 desviación estándar o menos por debajo de la media (panel a), y una probabilidad del 34.1% de que una observación sea mayor que una desviación estándar por debajo de la media pero menor que la media (panel b). Si sumamos estos dos valores, obtendremos 15.9% + 34.1% = 50%. Para datos que estén normalmente distribuidos, existe un 50% de probabilidad de que una observación caiga por debajo de la media. Esto implica que existe un 50% de probabilidad de que caiga por encima de la media.
</p>
</div>
<div id="density" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Densidad de probabilidad</h3>
<p>A lo largo de la discusión sobre la distribución normal, ha habido un par de cosas que parecen no tener sentido. Quizás hayas notado que el eje <span class="math inline">\(y\)</span> en estas Figuras se denomina como “Densidad de probabilidad” en lugar de “Probabilidad”. Tal vez notaste que utilizamos <span class="math inline">\(p(X)\)</span> en lugar de <span class="math inline">\(P(X)\)</span> en la fórmula de la distribución normal.</p>
<p>Si utilizamos la Figura y calculamos (siguiendo la fórmula) la probabilidad de <code>x = 1</code>, para una variable normalmente distribuida con <code>media = 1</code> y desviación estándar <code>sd = 0.1</code>, nos arrojará como resultado una probabilidad de 3.99. Sin embargo, hemos visto anteriormente que las probabilidades <em>no</em> pueden ser mayores que 1. Entonces, ¿qué es lo que hemos calculado?</p>
<p>Lo que hemos calculado aquí en realidad no es una probabilidad: Para entender qué es ese algo, tenemos que pensar qué es lo que realmente <em>significa</em> decir que <span class="math inline">\(X\)</span> es una variable continua. Digamos que estamos hablando de la temperatura otra vez. El termómetro me dice que hacen 23 grados, pero yo sé que eso no es del todo cierto. No hacen 23 grados <em>exactamente</em>. Quizás sea algo más cercano a los 23.1 grados o, si seguimos, en realidad podrían ser 23.095 grados. Esto es lo que sucede con los valores continuos: nunca se sabe el valor exacto.</p>
<p>Ahora pensemos en lo que esto implica cuando hablamos de probabilidades. Supongamos la temperatura máxima para mañana se toma de una distribución normal con media 23 y desviación estándar 1. ¿Cuál es la probabilidad de que la temperatura sea <em>exactamente</em> 23 grados? La respuesta es “cero”, o posiblemente, “un número tan cercano a cero que bien podría ser cero”. ¿Por qué es esto? Es como intentar tirar un dardo en un tablero de dardos con dianas infinitamente cada vez más pequeñas: no importa cuán buena sea tu puntería, nunca acertarás. En la vida real nunca obtendremos el valor exacto de 23. Siempre será 23.1 o 22.99998 o algo así. En en otras palabras, no tiene sentido hablar de la probabilidad de que la temperatura sea exactamente 23 grados. Sin embargo, en el día a día, si el termómetro indica 23 grados pero en realidad hacen 22.9998 grados, probablemente no nos importe demasiado. Esto es porque en el día a día, “23 grados” por lo general significa algo así como “en algún lugar entre 22.5 y 23.5 grados”. Y aunque no parezca muy importante preguntar por la probabilidad de que la temperatura sea exactamente 23 grados, lo que sí lo parece es preguntar sobre la probabilidad de que la temperatura se encuentre entre 22.5 y 23.5, o entre 20 y 30, o cualquier otro rango de temperaturas en el que estemos interesados.</p>
<p>El objetivo de esta explicación es dejar claro que, cuando hablamos de distribuciones continuas, no tiene sentido hablar sobre la probabilidad de un valor específico. Sin embargo, sí que <em>podemos</em> hablar sobre la probabilidad de que el valor se encuentre dentro de un rango particular de valores. Para encontrar probabilidad asociada con un rango particular, lo que debe hacer es calcular el “área bajo la curva”. Este concepto lo conocemos: en la Figura <a href="probability.html#fig:sdnorm1">1.9</a>, las áreas sombreadas representan probabilidades genuinas (por ejemplo, la Figura <a href="probability.html#fig:sdnorm1">1.9</a> muestra la probabilidad de observar un valor que cae dentro de 1 desviación estándar sobre la media).</p>
<p>Para finalizar, volveremos con la fórmula para <span class="math inline">\(p(x)\)</span> que vimos anteriormente. Los resultados de <span class="math inline">\(p(x)\)</span> no describe una probabilidad, sino una <strong><em>densidad de probabilidad</em></strong>, que en las gráficas corresponde a la altura de la curva. De la misma forma en que las probabilidades son números no-negativos que deben sumar 1, las densidades de probabilidad son números no-negativos que deben integrar a 1 (donde la integral se toma a través de todos los valores posibles de <span class="math inline">\(X\)</span>). Para calcular la probabilidad de que <span class="math inline">\(X\)</span> caiga entre <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> calculamos la integral definida de la función de densidad sobre el rango correspondiente, <span class="math inline">\(\int_a^b p(x) \ dx\)</span>. Se trata simplemente de otra forma de llegar al mismo resultado.</p>
</div>
</div>
<div id="otherdists" class="section level2">
<h2><span class="header-section-number">1.6</span> Otras distribuciones útiles</h2>
<p>La distribución normal es la distribución más utilizada por los estadísticos (por razones que se discutirán más adelante), y la distribución binomial es útil muchos escenarios. Sin embargo, existen otros tipos de distribuciones de probabilidad. Revisaremos brevemente 3 de ellas: la distribución <span class="math inline">\(t\)</span>, la distribución <span class="math inline">\(\chi^2\)</span> y la distribución <span class="math inline">\(F\)</span>. La <strong><em>distribución <span class="math inline">\(t\)</span></em></strong> es una distribución continua que se parece mucho a una distribución normal, pero que tiene colas más pesadas (ver Figura <a href="probability.html#fig:tdist">1.11</a>). Esta distribución tiende a surgir en situaciones en las que piensa que los datos siguen una distribución normal, pero no se conoce la media o la desviación estándar.</p>
<div class="figure"><span id="fig:tdist"></span>
<img src="FdI2_files/figure-html/tdist-1.png" alt="Una distribución $t$ con 3 grados de libertad (línea continua). Se asemeja a una distribución normal, pero no es igual (línea discontinua). Ten en cuenta que las &quot;colas&quot; de la distribución $t$ son más &quot;pesadas&quot;  (es decir, se extienden más hacia afuera, conteniendo más valores que se alejan de la media) que las colas de la distribución normal." width="672" />
<p class="caption">
Figure 1.11: Una distribución <span class="math inline">\(t\)</span> con 3 grados de libertad (línea continua). Se asemeja a una distribución normal, pero no es igual (línea discontinua). Ten en cuenta que las “colas” de la distribución <span class="math inline">\(t\)</span> son más “pesadas” (es decir, se extienden más hacia afuera, conteniendo más valores que se alejan de la media) que las colas de la distribución normal.
</p>
</div>
<ul>
<li>La <strong><em>distribución <span class="math inline">\(\chi^2\)</span></em></strong> es otra distribución que podemos encontrar con cierta frecuencia. Es habitual encontrarla cuando hacemos análisis de datos categóricos. Los valores de una distribución <span class="math inline">\(\chi^2\)</span> se consiguen al elevar al cuadrado los valores de una variable distribuída normalmente y luego sumarlos (un procedimiento denominado “suma de cuadrados”). Después veremos porqué es útil hacer una “suma de cuadrados”. La apariencia de una distribución <span class="math inline">\(\chi^2\)</span> la puedes encontrar en la Figura <a href="probability.html#fig:chisqdist">1.12</a>.</li>
</ul>
<div class="figure"><span id="fig:chisqdist"></span>
<img src="FdI2_files/figure-html/chisqdist-1.png" alt="Una distribución $chi^2$ con 3 grados de libertad (3 repeticiones, lo explicaremos más adelante). Observa que los valores siempre deben ser mayores que cero (los valores se elevan al cuadrado y se suman), y que la distribución es bastante sesgada (en este caso hacia la derecha). Estas son las características clave de una distribución chi-cuadrado." width="672" />
<p class="caption">
Figure 1.12: Una distribución <span class="math inline">\(chi^2\)</span> con 3 grados de libertad (3 repeticiones, lo explicaremos más adelante). Observa que los valores siempre deben ser mayores que cero (los valores se elevan al cuadrado y se suman), y que la distribución es bastante sesgada (en este caso hacia la derecha). Estas son las características clave de una distribución chi-cuadrado.
</p>
</div>
<ul>
<li>La <strong><em>distribución <span class="math inline">\(F\)</span></em></strong> se parece un poco a la distribución <span class="math inline">\(\chi^2\)</span> y surge cada vez que necesitamos comparar dos distribuciones <span class="math inline">\(\chi^2\)</span> entre sí. Es decir, si queremos comparar dos “sumas de cuadrados” diferentes, nos encontraremos con una distribución <span class="math inline">\(F\)</span>. Aún no hemos visto un ejemplo de todo lo que implica una suma de cuadrados, pero lo veremos cuando hablemos sobre ANOVAs, donde nos encontraremos nuevamente con la distribución <span class="math inline">\(F\)</span>.</li>
</ul>
<div class="figure"><span id="fig:Fdist"></span>
<img src="FdI2_files/figure-html/Fdist-1.png" alt="Una distribución $F$ con 3 y 5 grados de libertad. Cualitativamente hablando, es similar a una distribución de chi-cuadrado, pero por lo general el significado no es el mismo." width="672" />
<p class="caption">
Figure 1.13: Una distribución <span class="math inline">\(F\)</span> con 3 y 5 grados de libertad. Cualitativamente hablando, es similar a una distribución de chi-cuadrado, pero por lo general el significado no es el mismo.
</p>
</div>
<p>Debido a que estas distribuciones están estrechamente relacionadas con la distribución normal y entre sí, y porque se convertirán en las distribuciones importantes al hacer análisis estadísticos inferenciales en este curso, creo que es útil hacer una pequeña demostración de cómo estas distribuciones realmente están relacionadas entre sí. Primero, imagina que tenemos un conjunto de 1,000 observaciones aleatorias distribuidas normalmente al cual llamaremos “Muestra A”.</p>
<p>Esta “Muestra A” es una variable que contiene 1,000 números que se distribuyen normalmente y tienen una media de 0 y desviación estándar de 1. En la Figura <a href="probability.html#fig:distnormal">1.14</a> podemos ver un histograma con la distribución de los valores organizados por columnas, así como una línea negra sólida que representa la distribución verdadera de los datos (es decir, una distribución normal con valores infinitos con media 0 y desviación estándar 1). Así podemos comparar los datos recién generados con los de una distribución normal verdadera.</p>
<div class="figure"><span id="fig:distnormal"></span>
<img src="FdI2_files/figure-html/distnormal-1.png" alt="Distribución normal de la Muestra A (histograma), junto con la distribución normal verdadera (línea sólida)" width="672" />
<p class="caption">
Figure 1.14: Distribución normal de la Muestra A (histograma), junto con la distribución normal verdadera (línea sólida)
</p>
</div>
<p>En la Figura anterior podemos observar cómo han sido generados muchos valores distribuidos normalmente que luego han sido comparados con la distribución de probabilidad verdadera (línea sólida). Supongamos que ahora queremos una distribución chi-cuadrada con 3 grados de libertad. Como hemos mencionado anteriormente, una distribución chi-cuadrada con <span class="math inline">\(k\)</span> grados de libertad es es el resultado de tomar <span class="math inline">\(k\)</span> variables (o muestras) normalmente distribuidas (con media 0 y desviación estándar 1), elevarlas al cuadrado y sumarlas. Como queremos una distribución de chi-cuadrada con 3 grados de libertad, además de nuestra “Muestra A”, necesitamos dos conjuntos más de valores (también distribuidos normalmente). A estas nuevas dos variables las llamaremos “Muestra B” y “Muestra C”:</p>
<div class="figure"><span id="fig:distchi"></span>
<img src="FdI2_files/figure-html/distchi-1.png" alt="Distribución chi-cuadrada. Incluye a las Muestras A, B y C (3 grados de libertad)" width="672" />
<p class="caption">
Figure 1.15: Distribución chi-cuadrada. Incluye a las Muestras A, B y C (3 grados de libertad)
</p>
</div>
<p>Una vez que tenemos las tres variables, la teoría dice que debemos elevarlos al cuadrado y sumarlos, con lo que obtendremos 1,000 observaciones que siguen una distribución de chi-cuadrada con 3 grados de libertad. Visualmente, obtendremos una distribución como en la Figura <a href="probability.html#fig:distchi">1.15</a>.</p>
<p>Podemos extender esta demostración y tratar de entender el origen de la distribución <span class="math inline">\(t\)</span> y la distribución <span class="math inline">\(F\)</span>. Antes, hemos dicho que la distribución <span class="math inline">\(t\)</span> está relacionada con la distribución normal cuando se desconoce la media o la desviación estándar. Sin embargo, existe una relación más precisa entre las distribuciones normal, chi-cuadrada y <span class="math inline">\(t\)</span>. Supongamos que “escalamos” nuestros datos anteriores de la chi-cuadrada al dividirla entre sus 3 grados de libertad.</p>
<p>Si tomamos un conjunto de variables normalmente distribuidas (pensemos ahora en una “Muestra D”) y las dividimos por (la raíz cuadrada de) nuestra variable chi-cuadrada “escalada” que tenía <span class="math inline">\(k=3\)</span> grados de libertad, la operación dará como resultado una distribución <span class="math inline">\(t\)</span> con 3 grados de libertad. Si trazamos el histograma de esta nueva distribución <span class="math inline">\(t\)</span>, observaremos algo parecido al de la Figura <a href="probability.html#fig:distt">1.16</a>.</p>
<div class="figure"><span id="fig:distt"></span>
<img src="FdI2_files/figure-html/distt-1.png" alt="Distribución t. Es el resultado de dividir una distribución normal (en este caso la Muestra D) entre una variable chi-cuadrada escalada" width="672" />
<p class="caption">
Figure 1.16: Distribución t. Es el resultado de dividir una distribución normal (en este caso la Muestra D) entre una variable chi-cuadrada escalada
</p>
</div>
<p>Del mismo modo, podemos obtener una distribución <span class="math inline">\(F\)</span> al dividir dos distribuciones chi-cuadrada “escaladas”. Supongamos, por ejemplo, que deseamos generar datos que sigan una distribución <span class="math inline">\(F\)</span> con 3 y 20 grados de libertad (es decir, con 3 y 20 variables respectivamente). La división de los valores de ambas distribuciones nos da como resultado una nueva variable <code>F.3.20</code> y su distribución es la que se muestra en la Figura <a href="probability.html#fig:distf">1.17</a>.</p>
<div class="figure"><span id="fig:distf"></span>
<img src="FdI2_files/figure-html/distf-1.png" alt="Distribución F. En este ejemplo hipotético, se compara la distribución chi-cuadrada de 3 grados de libertad previa con otra distribución chi-cuadrada con 20 grados de libertad (es decir, que incluye 20 muestras o variables)" width="672" />
<p class="caption">
Figure 1.17: Distribución F. En este ejemplo hipotético, se compara la distribución chi-cuadrada de 3 grados de libertad previa con otra distribución chi-cuadrada con 20 grados de libertad (es decir, que incluye 20 muestras o variables)
</p>
</div>
<p>Hemos visto tres nuevas distribuciones: <span class="math inline">\(\chi^2\)</span>, <span class="math inline">\(t\)</span> y <span class="math inline">\(F\)</span>. Todas son distribuciones continuas, y todas están estrechamente relacionadas con la distribución normal. Hemos hablado un poco poco sobre la naturaleza de esta relación. Sin embargo, la clave no es que tengas una comprensión profunda y detallada de todas estas diferentes distribuciones, ni que recuerdes las relaciones precisas que existen entre ellas. Lo más importante es entender la idea básica de que estas distribuciones están profundamente relacionadas entre sí y a su vez con la distribución normal. Más adelante en el curso nos vamos a encontrar con datos que se distribuyen normalmente, o que al menos suponemos que se distribuyen normalmente. Por lo tanto, si suponemos que nuestros datos se distribuyen normalmente, debemos saber reconocer las distribuciones <span class="math inline">\(\chi^2\)</span>, <span class="math inline">\(t\)</span> y <span class="math inline">\(F\)</span>..</p>
</div>
<div id="resumen" class="section level2">
<h2><span class="header-section-number">1.7</span> Resumen</h2>
<p>En este capítulo hemos hablado de probabilidad. Hemos hablado de lo que significa la probabilidad y por qué los estadísticos no están muy de acuerdo en lo que significa. Hablamos sobre las reglas que las probabilidades tienen que obedecer. Hemos introducido la idea de una distribución de probabilidad y conocido algunas de las distribuciones de probabilidad más importantes con las que nos podemos encontrar. Los temas han sido los siguientes:</p>
<ul>
<li>Teoría de probabilidad vs estadística (Sección <a href="probability.html#probstats">1.1</a>)</li>
<li>Visión frecuenciantista vs visión bayesiana de probabilidad (Sección <a href="probability.html#probmeaning">1.2</a>)</li>
<li>Conceptos básicos de la teoría de probabilidad (Sección <a href="probability.html#basicprobability">1.3</a>)</li>
<li>Distribución binomial (Sección <a href="probability.html#binomial">1.4</a>), distribución normal (sección <a href="probability.html#normal">1.5</a>), y otras distribuciones (Sección <a href="probability.html#otherdists">1.6</a>)</li>
</ul>
<p>Esto es una simple introducción dentro de un gran tema. La teoría de la probabilidad es una rama de las matemáticas, completamente separada de su aplicación a la estadística y al análisis de datos. Como tales, hay miles de libros escritos sobre el tema y las universidades generalmente ofrecen clases dedicadas por completo a la teoría de la probabilidad. En este capítulo se han descrito cinco distribuciones de probabilidad estándar, pero existen <em>muchas</em> más que esas. Afortunadamente, estas distribuciones bastarán por el momento.</p>
<p>Los conceptos básicos que hemos adquirido en este capítulo servirán como fundamento para los siguientes dos. Existen muchas reglas sobre lo que se nos “permite” decir cuando hacemos inferencia estadística, y muchas de ellas pueden parecer arbitrarias. Sin embargo, veremos que comienzan a tener sentido si tenemos en cuenta estos conceptos básicos que hemos aprendido.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Hay que tener en cuenta que el término “éxito” es bastante arbitrario, y en realidad no implica que el resultado sea algo deseado. Si <span class="math inline">\(\theta\)</span> se refiriera a la probabilidad de que un pasajero se lesione en un accidente de autobús, seguiría siendo una probabilidad de éxito, aunque en realidad no queremos que la gente salga lastimada<a href="probability.html#fnref1">↩</a></p></li>
<li id="fn2"><p>En la práctica, la distribución normal es tan útil que las personas tienden a usarla incluso cuando la variable no es continua. Siempre que haya suficientes categorías (por ejemplo, respuestas de escala Likert de un cuestionario), suele ser frecuente el uso de la distribución normal.<a href="probability.html#fnref2">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="estimation.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["FdI2.pdf", "FdI2.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
